# -*- coding: utf-8 -*-
"""Resume_Ranker(Spacy).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s5Z1pRYMxCB0CVl3hQ-RfX8T4GliG3Mk
"""

!pip install -U spacy
!pip install spacy_transformers

!pip install pyMuPDF

import spacy
from spacy.tokens import DocBin
from tqdm import tqdm
import json
from collections import defaultdict 
import numpy as np
import sys, fitz
import locale

def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

from google.colab import drive
drive.mount('/content/drive')

spacy.__version__

!nvidia-smi

cv_data=json.load(open("/content/drive/MyDrive/Resume_parser_files/Dataset/dataset.json", 'r'))

len(cv_data)

cv_data[0]

#!python -m spacy init fill-config /content/drive/MyDrive/Resume_parser_files/Config/base_config.cfg /content/drive/MyDrive/Resume_parser_files/Config/config.cfg

def get_spacy_doc(file, data):
  nlp= spacy.blank("en")
  db=DocBin()   

  for text, annot in tqdm(data):        #text and annotated data 
    doc= nlp.make_doc(text)           #creates a doc from the text
    annot= annot['entities']

    ents=[]
    entity_indices=[]

    for start, end, label in annot:
      skip_entity= False
      for idx in range(start, end):
        if idx in entity_indices:
          skip_entity= True          #overlapping of entities
          break
      if skip_entity == True:
        continue


      entity_indices=entity_indices + list(range(start, end))     #adding index of entities 

      try:                  #get the text 
        span= doc.char_span(start,end, label=label, alignment_mode='strict')  
      except:
        continue
        
      if span is None:        #if error// start and end is same
        err_data= str([start,end])+ "     "+  str(text)  +"\n"
        file.writer(err_data)

      else:
        ents.append(span)
        
    try:
      doc.ents= ents
      db.add(doc)
    except:
      pass

  return db

from sklearn.model_selection import train_test_split
train,test= train_test_split(cv_data, test_size=0.3)

len(train), len(test)

file = open('/content/drive/MyDrive/Resume_parser_files/Model/Resume/train_file.txt', 'w')

db = get_spacy_doc(file, train)
db.to_disk('/content/drive/MyDrive/Resume_parser_files/Model/Resume/train_data.spacy')

db= get_spacy_doc(file, test)
db.to_disk('/content/drive/MyDrive/Resume_parser_files/Model/Resume/test_data.spacy')
file.close()

# !python -m spacy train /content/drive/MyDrive/Resume_parser_files/Config/config.cfg --output /content/drive/MyDrive/Resume_parser_files/Model/Resume/output --paths.train /content/drive/MyDrive/Resume_parser_files/Model/Resume/train_data.spacy --paths.dev /content/drive/MyDrive/Resume_parser_files/Model/Resume/test_data.spacy --gpu-id 0

"""# Extracting Text from Resume"""

resume_model=spacy.load("/content/drive/MyDrive/Resume_parser_files/Model/Resume/output/model-best")      #best model

resume_path= "/content/drive/MyDrive/Resume_parser_files/Test_data/Resumes/Shreya_Bhandari.pdf"
resume= fitz.open(resume_path)

#extract text from cv
text1=""
for page in resume:
  text1 = text1+ str(page.get_text())

#text1
print(text1)

resume= resume_model(text1)
for ent in resume.ents:
  print(ent.text ," -->>>>>> " , ent.label_)

resume_features=defaultdict(list)

for ent in resume.ents:
  resume_features[ent.label_].append(ent.text)

resume_features

"""# Job Description"""

jd_model=spacy.load("/content/drive/MyDrive/Resume_parser_files/Model/Jd/output/model-best")

jd_path= "/content/drive/MyDrive/Resume_parser_files/Test_data/Job_Description/python_developer.pdf"
jd= fitz.open(jd_path)

text2=" "
for page in jd:
  text2= text2+ str(page.get_text())

print(text2)

job_post=defaultdict(list)

jd= jd_model(text2)
for ent in jd.ents:
  job_post[ent.label_].append(ent.text)

print(type(jd.ents))

job_post

cv_skills= [x.casefold() for x in resume_features['SKILLS']]
cv_worked_as= [x.casefold() for x in resume_features['WORKED AS']]
cv_exp= [x.casefold() for x in resume_features['YEARS OF EXPERIENCE']]
cv_companies= [x.casefold() for x in resume_features['COMPANIES WORKED AT']]
cv_degree= [x.casefold() for x in resume_features['DEGREE']]

jd_skills= [x.casefold() for x in job_post['SKILLS']]
jd_degree= [x.casefold() for x in job_post['DEGREE']]
jd_jobpost= [x.casefold() for x in job_post['JOBPOST']]
jd_exp= [x.casefold() for x in job_post['EXPERIENCE']]

cv_skills

companies_list=['apple', 
                'microsoft', 
                'facebook', 
                'danson solutions',
                'girls in tech',
                'dansontraining',
                'finnove technologies',
                'danson solutions',
                'chimpvine',
                'vertexreport',
                'vedic himalayan adve',
                'tcS',
                'wipro']

jd_skills

score=0
#Match Skill
skill_score = sum([5 for skill in cv_skills if skill in jd_skills])
score += skill_score
#Check Degree
degree_score= sum([10 for deg in cv_degree if deg in jd_degree])
score +=degree_score
#Check early exp
work_score= sum([10 for work in cv_worked_as if work in jd_jobpost])
score += work_score
#check companies worked in
company_score= sum([10 for comp in cv_companies if comp in companies_list])
score += company_score

score

'''
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def compare_lists(list1, list2):

    # Combine the two lists into a single list of strings
    combined_text = list1 + list2

    # Create a CountVectorizer object to convert the text into a matrix of token counts
    vectorizer = CountVectorizer().fit_transform(combined_text)

    # Calculate the cosine similarity between the two lists
    cosine_sim = cosine_similarity(vectorizer[0:len(list1)], vectorizer[len(list1):])[0][0]

    return cosine_sim

    '''

'''Percentile_Score='''

'''# Calculate the experience score
experience_score = 0
for exp in cv_exp:
  for req_exp in jd_exp:
    if req_exp in exp:
      experience_score += 1
      break
score += experience_score'''













